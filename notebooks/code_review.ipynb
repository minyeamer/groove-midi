{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Review\n",
    "- 전처리 -> 학습 -> 생성을 프로세스로 인식하고 각각에 대응되는 소스코드를 순서대로 탐색합니다.\n",
    "- `music_vae_train.py`에서 `data.py`와 관련된 부분을 집중적으로 찾아보면서 전처리 과정을 탐색합니다. (08-20 08:00-09:00, 1 hour)\n",
    "- `music_vae_train.py`에서 top-down 방식으로 전체적인 학습 과정을 탐색합니다. (08-20 10:00-12:00, 2 hours)\n",
    "- `music_vae_generate.py`에서 top-down 방식으로 전체적인 생성 과정을 탐색합니다. (08-20 12:00-13:00, 1 hour)\n",
    "- 과제 수행에 있어서 custom config를 사용해야할 필요성을 느끼고 해당 부분을 다시 확인합니다. (08-20 15:00-17:00, 2 hours)\n",
    "- 전처리와 학습 과정에서 지나쳤던 부분들을 다시 점검합니다. (08-21 08:00-10:00, 2 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 The Magenta Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-style:dotted\">\n",
    "\n",
    "### 전처리 과정 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# music_vae_train.py\n",
    "\n",
    "from magenta.models.music_vae import data\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tf_slim\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def train():\n",
    "  pass\n",
    "\n",
    "def run(config_map,\n",
    "        tf_file_reader=tf.data.TFRecordDataset,\n",
    "        file_reader=tf.python_io.tf_record_iterator):\n",
    "  config = config_map[FLAGS.config]\n",
    "\n",
    "  if FLAGS.mode == 'train':\n",
    "    is_training = True\n",
    "  elif FLAGS.mode == 'eval':\n",
    "    is_training = False\n",
    "\n",
    "  def dataset_fn():\n",
    "    return data.get_dataset(\n",
    "        config,\n",
    "        tf_file_reader=tf_file_reader,\n",
    "        is_training=is_training,\n",
    "        cache_dataset=FLAGS.cache_dataset)\n",
    "\n",
    "  if is_training:\n",
    "    train(\n",
    "        # ...\n",
    "        dataset_fn=dataset_fn,\n",
    "        # ...\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `music_vae_train.py`에서 `data.py`를 호출하는 부분은 `dataset_fn()` 내 `get_dataset()`이 유일하기 때문에,   \n",
    "  해당 함수를 메인으로 판단하고 시작점으로서 탐색을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.py\n",
    "\n",
    "def get_dataset(\n",
    "    config,\n",
    "    tf_file_reader=tf.data.TFRecordDataset,\n",
    "    is_training=False,\n",
    "    cache_dataset=True):\n",
    "\n",
    "  batch_size = config.hparams.batch_size\n",
    "  examples_path = (\n",
    "      config.train_examples_path if is_training else config.eval_examples_path)\n",
    "  note_sequence_augmenter = (\n",
    "      config.note_sequence_augmenter if is_training else None)\n",
    "  data_converter = config.data_converter\n",
    "  data_converter.set_mode('train' if is_training else 'eval')\n",
    "\n",
    "  if examples_path:\n",
    "    tf.logging.info('Reading examples from file: %s', examples_path)\n",
    "    num_files = len(tf.gfile.Glob(examples_path))\n",
    "    if not num_files:\n",
    "      raise ValueError(\n",
    "          'No files were found matching examples path: %s' %  examples_path)\n",
    "    files = tf.data.Dataset.list_files(examples_path)\n",
    "    dataset = files.interleave(\n",
    "        tf_file_reader,\n",
    "        cycle_length=tf.data.experimental.AUTOTUNE,\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  elif config.tfds_name:\n",
    "    pass\n",
    "  # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `get_dataset()` 함수의 파라미터로 `tf.data.TFRecordDataset` 타입의 객체가 요구되는 것으로 보아,   \n",
    "  MIDI 파일이 아닌, TFRecord 파일로 변환된 데이터를 읽어오는 동작을 짐작해볼 수 있습니다.\n",
    "- `get_dataset()` 함수에서는 실제로 `config`로부터 `batch_size`, `train_examples_path`, `data_converter`, `mode` 정보를 가져와서,   \n",
    "  대상 경로가 존재하면 TFRecord 파일을 읽어오고, 존재하지 않으면 `tfds_name`으로 설정된 경로에서 MIDI 파일을 읽어 TFRecord로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# music_vae_train.py\n",
    "\n",
    "def train(config, dataset_fn):\n",
    "  # ...\n",
    "  with tf.Graph().as_default():\n",
    "    # ...\n",
    "    model = config.model\n",
    "    # ...\n",
    "    optimizer = model.train(**_get_input_tensors(dataset_fn(), config))\n",
    "    # ...\n",
    "\n",
    "def _get_input_tensors(dataset, config):\n",
    "  \"\"\"Get input tensors from dataset.\"\"\"\n",
    "  batch_size = config.hparams.batch_size\n",
    "  iterator = tf.data.make_one_shot_iterator(dataset)\n",
    "  (input_sequence, output_sequence, control_sequence,\n",
    "   sequence_length) = iterator.get_next()\n",
    "  input_sequence.set_shape(\n",
    "      [batch_size, None, config.data_converter.input_depth])\n",
    "  output_sequence.set_shape(\n",
    "      [batch_size, None, config.data_converter.output_depth])\n",
    "  if not config.data_converter.control_depth:\n",
    "    control_sequence = None\n",
    "  else:\n",
    "    control_sequence.set_shape(\n",
    "        [batch_size, None, config.data_converter.control_depth])\n",
    "  sequence_length.set_shape([batch_size] + sequence_length.shape[1:].as_list())\n",
    "\n",
    "  return {\n",
    "      'input_sequence': input_sequence,\n",
    "      'output_sequence': output_sequence,\n",
    "      'control_sequence': control_sequence,\n",
    "      'sequence_length': sequence_length\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `TFRecordDataset`을 반환하는 해당 함수는 `music_vae_train` 모듈 내 `train()` 함수의 파라미터로 전달되며,   \n",
    "  학습 단계에서 `_get_input_tensors()` 함수를 통해 batch size만큼의 시퀀스가 담긴 dictionary를 반환하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-style:dotted\">\n",
    "\n",
    "### 학습 과정 탐색\n",
    "- `music_vae_train.py`를 통한 학습 수행을 위해 파이썬 스크립트 실행 과정에서 `mode=train`으로 설정하면,   \n",
    "  내부적으로 `boolean` 타입의 `is_training` 변수 값을 설정해 `train()` 함수를 호출할지 `evaluate()` 함수를 호출할지 결정합니다.\n",
    "- `train()` 함수는 `config`에서 `run_dir`로 설정된 디렉토리 아래 `train` 위치를 `train_dir`로서 전달받고,   \n",
    "  그 외에 전체 config 정보, 데이터셋을 불러오는 함수 `dataset_fn()`, 기타 FLAGS 설정을 파라미터로 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs.py\n",
    "\n",
    "import collections\n",
    "from magenta.common import merge_hparams\n",
    "from magenta.contrib import training as contrib_training\n",
    "from magenta.models.music_vae import data\n",
    "from magenta.models.music_vae import lstm_models\n",
    "from magenta.models.music_vae.base_model import MusicVAE\n",
    "\n",
    "HParams = contrib_training.HParams\n",
    "\n",
    "\n",
    "class Config(collections.namedtuple(\n",
    "    'Config',\n",
    "    ['model', 'hparams', 'note_sequence_augmenter', 'data_converter',\n",
    "     'train_examples_path', 'eval_examples_path', 'tfds_name'])):\n",
    "\n",
    "  def values(self):\n",
    "    return self._asdict()\n",
    "\n",
    "CONFIG_MAP = {}\n",
    "\n",
    "CONFIG_MAP['cat-drums_2bar_small'] = Config(\n",
    "    model=MusicVAE(lstm_models.BidirectionalLstmEncoder(),\n",
    "                   lstm_models.CategoricalLstmDecoder()),\n",
    "    hparams=merge_hparams(\n",
    "        lstm_models.get_default_hparams(),\n",
    "        HParams(\n",
    "            batch_size=512,\n",
    "            # ...\n",
    "        )),\n",
    "    note_sequence_augmenter=None,\n",
    "    data_converter=data.DrumsConverter(\n",
    "        max_bars=100,  # Truncate long drum sequences before slicing.\n",
    "        slice_bars=2,\n",
    "        steps_per_quarter=4,\n",
    "        roll_input=True),\n",
    "    train_examples_path=None,\n",
    "    eval_examples_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `train()` 함수를 알아보기에 앞서, `config`에 어떤 정보가 담겨있는지 보기 위해 `configs.py`를 탐색하기로 했고,   \n",
    "  4마디에 해당하는 드럼 샘플을 추출하기 위한 목적과 가장 유사한 설정 정보인 `cat-drums_2bar_small`를 중심으로 알아봅니다.\n",
    "- 해당하는 config에 저장된 것에는 `model`, `hparams`, `data_converter`, `train_examples_path` 등이 있는데,   \n",
    "  `hparams`는 이름에서 보듯이 하이퍼파라미터 설정이고 `train_examples_path`는 TFRecord 파일에 대한 경로임을 앞선 과정을 통해 알 수 있습니다.\n",
    "- 다만, 해당 설정에서 보이는 모델의 구조는 계층적 구조로 이루어지지 않은,   \n",
    "  위 논문에서 정의된 flat baseline에 해당하는 모델로 예상되기 때문에 계층적 디코더 모델에 대한 설정을 같이 확인해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.py\n",
    "\n",
    "import note_seq\n",
    "from note_seq import drums_encoder_decoder\n",
    "import numpy as np\n",
    "\n",
    "REDUCED_DRUM_PITCH_CLASSES = drums_encoder_decoder.DEFAULT_DRUM_TYPE_PITCHES\n",
    "OUTPUT_VELOCITY = 80\n",
    "\n",
    "\n",
    "class BaseNoteSequenceConverter(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "class DrumsConverter(BaseNoteSequenceConverter):\n",
    "    \"\"\"Converter for legacy drums with either pianoroll or one-hot tensors.\"\"\"\n",
    "\n",
    "    def __init__(self, max_bars=None, slice_bars=None, pitch_classes=None,):\n",
    "        self._pitch_classes = pitch_classes or REDUCED_DRUM_PITCH_CLASSES\n",
    "        self._pitch_class_map = {}\n",
    "        # ...\n",
    "        num_classes = len(self._pitch_classes)\n",
    "        # ...\n",
    "        self._oh_encoder_decoder = note_seq.MultiDrumOneHotEncoding(\n",
    "            drum_type_pitches=[(i,) for i in range(num_classes)])\n",
    "\n",
    "    def from_tensors(self, samples, unused_controls=None):\n",
    "        output_sequences = []\n",
    "        for s in samples:\n",
    "            # ...\n",
    "            s = np.argmax(s, axis=-1)\n",
    "            if self.end_token is not None and self.end_token in s:\n",
    "                s = s[:s.tolist().index(self.end_token)]\n",
    "                events_list = [self._oh_encoder_decoder.decode_event(e) for e in s]\n",
    "        events_list = [\n",
    "            frozenset(self._pitch_classes[c][0] for c in e) for e in events_list]\n",
    "        track = note_seq.DrumTrack(\n",
    "            events=events_list,\n",
    "            steps_per_bar=self._steps_per_bar,\n",
    "            steps_per_quarter=self._steps_per_quarter)\n",
    "        output_sequences.append(track.to_sequence(velocity=OUTPUT_VELOCITY))\n",
    "        return output_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델을 알아보기 앞서 `data_converter`로 전달되는 `DrumsConverter`가 어떤 동작을 하는지 확인해보았습니다.\n",
    "- `Magenta`에서 MIDI 데이터를 처리하는 부분은 대부분이 `NoteSequence`라는 외부 모듈에 담겨있는데,   \n",
    "  그 중에서 드럼 데이터를 추출하는 것으로 예상되는 `MultiDrumOneHotEncoding`과 `decode_event` 메서드를 알아볼 필요가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note_seq/drums_encoder_decoder.py\n",
    "\n",
    "from note_seq import encoder_decoder\n",
    "\n",
    "\n",
    "class MultiDrumOneHotEncoding(encoder_decoder.OneHotEncoding):\n",
    "\n",
    "  def __init__(self, drum_type_pitches=None, ignore_unknown_drums=True):\n",
    "    self._drum_map = dict(enumerate(drum_type_pitches))\n",
    "    self._inverse_drum_map = dict((pitch, index)\n",
    "                                  for index, pitches in self._drum_map.items()\n",
    "                                  for pitch in pitches)\n",
    "    self._ignore_unknown_drums = ignore_unknown_drums\n",
    "\n",
    "  def decode_event(self, index):\n",
    "    bits = reversed(str(bin(index)))\n",
    "    # Use the first \"pitch\" for each drum type.\n",
    "    return frozenset(self._drum_map[i][0]\n",
    "                     for i, b in enumerate(bits)\n",
    "                     if b == '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `DrumsConverter`에서 `MultiDrumOneHotEncoding`를 생성할 때 클래스에 대한 2차원 배열을 전달하는데,   \n",
    "  해당 배열을 인덱스를 키값으로 하는 딕셔너리로 변환해 저장합니다.\n",
    "- `from_tensors()`에서 `events_list`를 생성하는 과정에서 `decode_event`가 호출되는데,   \n",
    "  각각의 이벤트 시퀀스에 대해 인덱스와 대응되는 클래스를 반환하는 과정이 진행되는 것이라 판단됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs.py\n",
    "\n",
    "CONFIG_MAP['hierdec-trio_16bar'] = Config(\n",
    "    model=MusicVAE(\n",
    "        lstm_models.BidirectionalLstmEncoder(),\n",
    "        lstm_models.HierarchicalLstmDecoder(\n",
    "            lstm_models.SplitMultiOutLstmDecoder(\n",
    "                core_decoders=[\n",
    "                    lstm_models.CategoricalLstmDecoder(),\n",
    "                    lstm_models.CategoricalLstmDecoder(),\n",
    "                    lstm_models.CategoricalLstmDecoder()],\n",
    "                output_depths=[\n",
    "                    90,  # melody\n",
    "                    90,  # bass\n",
    "                    512,  # drums\n",
    "                ]),\n",
    "            level_lengths=[16, 16],\n",
    "            disable_autoregression=True)),\n",
    "    # ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델에 대해 알아보기 위해 다시 config로 돌아와서, 16마디의 3채널 데이터에 대한 계층적 디코더 모델의 설정을 확인했을 때,   \n",
    "  `HierarchicalLstmDecoder`의 하위 계층 요소로 `CategoricalLstmDecoder`를 넣으면 이상적인 설정이 될 것이라 예상합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.py\n",
    "\n",
    "class MusicVAE(object):\n",
    "  \"\"\"Music Variational Autoencoder.\"\"\"\n",
    "\n",
    "  def __init__(self, encoder, decoder):\n",
    "    \"\"\"Initializer for a MusicVAE model.\"\"\"\n",
    "    self._encoder = encoder\n",
    "    self._decoder = decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `model`로 전달되는 객체인 `MusicVAE`는 `base_model.py`에서 확인할 수 있듯이 `encoder`와 `decoder`로 구성되며,   \n",
    "  각각 `BidirectionalLstmEncoder`, `HierarchicalLstmDecoder`를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_models.py\n",
    "\n",
    "import magenta.contrib.rnn as contrib_rnn\n",
    "import magenta.contrib.seq2seq as contrib_seq2seq\n",
    "from magenta.models.music_vae import base_model\n",
    "from magenta.models.music_vae import lstm_utils\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "class BidirectionalLstmEncoder(base_model.BaseEncoder):\n",
    "  \"\"\"Bidirectional LSTM Encoder.\"\"\"\n",
    "\n",
    "  def build(self, hparams, is_training=True, name_or_scope='encoder'):\n",
    "    self._is_training = is_training\n",
    "    self._name_or_scope = name_or_scope\n",
    "    # ...\n",
    "    self._cells = lstm_utils.build_bidirectional_lstm(\n",
    "        layer_sizes=hparams.enc_rnn_size,\n",
    "        dropout_keep_prob=hparams.dropout_keep_prob,\n",
    "        residual=hparams.residual_encoder,\n",
    "        is_training=is_training)\n",
    "\n",
    "  def encode(self, sequence, sequence_length):\n",
    "    cells_fw, cells_bw = self._cells\n",
    "\n",
    "    _, states_fw, states_bw = contrib_rnn.stack_bidirectional_dynamic_rnn(\n",
    "        cells_fw,\n",
    "        cells_bw,\n",
    "        sequence,\n",
    "        sequence_length=sequence_length,\n",
    "        time_major=False,\n",
    "        dtype=tf.float32,\n",
    "        scope=self._name_or_scope)\n",
    "    last_h_fw = states_fw[-1][-1].h\n",
    "    last_h_bw = states_bw[-1][-1].h\n",
    "\n",
    "    return tf.concat([last_h_fw, last_h_bw], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_utils.py\n",
    "\n",
    "rnn = tf.nn.rnn_cell\n",
    "\n",
    "def rnn_cell(rnn_cell_size, dropout_keep_prob, residual, is_training=True):\n",
    "  \"\"\"Builds an LSTMBlockCell based on the given parameters.\"\"\"\n",
    "  dropout_keep_prob = dropout_keep_prob if is_training else 1.0\n",
    "  cells = []\n",
    "  for i in range(len(rnn_cell_size)):\n",
    "    cell = contrib_rnn.LSTMBlockCell(rnn_cell_size[i])\n",
    "    # ...\n",
    "    cells.append(cell)\n",
    "  return rnn.MultiRNNCell(cells)\n",
    "\n",
    "def build_bidirectional_lstm(\n",
    "    layer_sizes, dropout_keep_prob, residual, is_training):\n",
    "  \"\"\"Build the Tensorflow graph for a bidirectional LSTM.\"\"\"\n",
    "\n",
    "  cells_fw = []\n",
    "  cells_bw = []\n",
    "  for layer_size in layer_sizes:\n",
    "    cells_fw.append(\n",
    "        rnn_cell([layer_size], dropout_keep_prob, residual, is_training))\n",
    "    cells_bw.append(\n",
    "        rnn_cell([layer_size], dropout_keep_prob, residual, is_training))\n",
    "\n",
    "  return cells_fw, cells_bw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `BidirectionalLstmEncoder`는 `build_bidirectional_lstm()` 함수를 통해 모델 구조를 생성하며,   \n",
    "  해당 함수는 내부적으로 forward, backward 방향에 대한 두 가지 RNN cell을 반환합니다.\n",
    "- 각각의 RNN cell은 RNN 구조를 상속받는 LSTM block으로 구성되며,   \n",
    "  하이퍼파라미터로 전달한 `enc_rnn_size`만큼의 초기화된 가중치를 가지고 있습니다.\n",
    "- 학습 과정에서 실행되는 `encode()` 구문에서는 양방향 RNN에서 각 방향의 마지막 hidden state를 병합한 결과를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.py\n",
    "\n",
    "ds = tfp.distributions\n",
    "\n",
    "\n",
    "class MusicVAE(object):\n",
    "  def encode(self, sequence, sequence_length, control_sequence=None):\n",
    "    \"\"\"Encodes input sequences into a MultivariateNormalDiag distribution.\"\"\"\n",
    "    hparams = self.hparams\n",
    "    z_size = hparams.z_size\n",
    "    sequence = tf.to_float(sequence)\n",
    "    #...\n",
    "    encoder_output = self.encoder.encode(sequence, sequence_length)\n",
    "\n",
    "    mu = tf.layers.dense(\n",
    "        encoder_output,\n",
    "        z_size,\n",
    "        name='encoder/mu',\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n",
    "    sigma = tf.layers.dense(\n",
    "        encoder_output,\n",
    "        z_size,\n",
    "        activation=tf.nn.softplus,\n",
    "        name='encoder/sigma',\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n",
    "\n",
    "    return ds.MultivariateNormalDiag(loc=mu, scale_diag=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 인코딩 과정은 `MusicVAE`의 `encode()` 메서드에서 `encoder_output`을 생성하기 위한 중간 과정이며,   \n",
    "  이후 해당 결과를 각각의 Dense 레이어를 거치게 하여 $\\mu$와 $\\sigma$를 생성합니다.\n",
    "- Reparameterization Trick이 적용된 별도 모듈을 사용해 $\\mu$와 $\\sigma$에 해당하는 정규 분포를 생성 및 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_models.py\n",
    "\n",
    "class HierarchicalLstmDecoder(base_model.BaseDecoder):\n",
    "  \"\"\"Hierarchical LSTM decoder.\"\"\"\n",
    "\n",
    "  def build(self, hparams, output_depth, is_training=True):\n",
    "    self.hparams = hparams\n",
    "    self._output_depth = output_depth\n",
    "    self._total_length = hparams.max_seq_len\n",
    "    #...\n",
    "    self._hier_cells = [\n",
    "        lstm_utils.rnn_cell(\n",
    "            hparams.dec_rnn_size,\n",
    "            dropout_keep_prob=hparams.dropout_keep_prob,\n",
    "            residual=hparams.residual_decoder)\n",
    "        # Subtract 1 for the core decoder level\n",
    "        for _ in range(len(self._level_lengths) - 1)]\n",
    "\n",
    "    with tf.variable_scope('core_decoder', reuse=tf.AUTO_REUSE):\n",
    "      self._core_decoder.build(hparams, output_depth, is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `HierarchicalLstmDecoder`는 하이퍼파라미터로 전달한 `level_lengths`개만큼의 LSTM block을 리스트로 가집니다.\n",
    "- 각각의 LSTM block은 마찬가지로 하이퍼파라미터로 전달한 `dec_rnn_size`만큼의 state로 구성되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_models.py\n",
    "\n",
    "class BaseLstmDecoder(base_model.BaseDecoder):\n",
    "  \"\"\"Abstract LSTM Decoder class.\"\"\"\n",
    "\n",
    "  def build(self, hparams, output_depth, is_training=True):\n",
    "    # ...\n",
    "    self._output_layer = tf.layers.Dense(\n",
    "        output_depth, name='output_projection')\n",
    "    self._dec_cell = lstm_utils.rnn_cell(\n",
    "        hparams.dec_rnn_size, hparams.dropout_keep_prob,\n",
    "        hparams.residual_decoder, is_training)\n",
    "\n",
    "  def _decode(self, z, helper, input_shape, max_length=None):\n",
    "    initial_state = lstm_utils.initial_cell_state_from_embedding(\n",
    "        self._dec_cell, z, name='decoder/z_to_initial_state')\n",
    "\n",
    "    decoder = lstm_utils.Seq2SeqLstmDecoder(\n",
    "        self._dec_cell,\n",
    "        helper,\n",
    "        initial_state=initial_state,\n",
    "        input_shape=input_shape,\n",
    "        output_layer=self._output_layer)\n",
    "    final_output, final_state, final_lengths = contrib_seq2seq.dynamic_decode(\n",
    "        decoder,\n",
    "        maximum_iterations=max_length,\n",
    "        swap_memory=True,\n",
    "        scope='decoder')\n",
    "    results = lstm_utils.LstmDecodeResults(\n",
    "        rnn_input=final_output.rnn_input[:, :, :self._output_depth],\n",
    "        rnn_output=final_output.rnn_output,\n",
    "        samples=final_output.sample_id,\n",
    "        final_state=final_state,\n",
    "        final_sequence_lengths=final_lengths)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `CategoricalLstmDecoder`는 base model을 상속받는 `HierarchicalLstmDecoder`와 다르게   \n",
    "  `BaseLstmDecoder`라는 추상 클래스를 상속받기 때문에, 해당하는 모델을 우선적으로 탐색했습니다.\n",
    "- `BaseLstmDecoder`는 내부적으로 단방향 LSTM block과 Dense 레이어로 구성된 구조입니다.\n",
    "- `BaseLstmDecoder`의 `decode()` 기능은 `Seq2Seq` 디코더의 동작 방식을 활용한 것으로 추정되며,   \n",
    "  해당 `dynamic_decode()` 함수는 `max_length`에 해당하는 `maximum_iterations`까지 반복문을 수행하면서 연속된 state를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_models.py\n",
    "\n",
    "class CategoricalLstmDecoder(BaseLstmDecoder):\n",
    "  \"\"\"LSTM decoder with single categorical output.\"\"\"\n",
    "\n",
    "  def _flat_reconstruction_loss(self, flat_x_target, flat_rnn_output):\n",
    "    flat_logits = flat_rnn_output\n",
    "    flat_truth = tf.argmax(flat_x_target, axis=1)\n",
    "    flat_predictions = tf.argmax(flat_logits, axis=1)\n",
    "    r_loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=flat_x_target, logits=flat_logits)\n",
    "\n",
    "    metric_map = {\n",
    "        'metrics/accuracy':\n",
    "            tf.metrics.accuracy(flat_truth, flat_predictions),\n",
    "        'metrics/mean_per_class_accuracy':\n",
    "            tf.metrics.mean_per_class_accuracy(\n",
    "                flat_truth, flat_predictions, int(flat_x_target.shape[-1])),\n",
    "    }\n",
    "    return r_loss, metric_map\n",
    "\n",
    "  def _sample(self, rnn_output, temperature=1.0):\n",
    "    sampler = tfp.distributions.OneHotCategorical(\n",
    "        logits=rnn_output / temperature, dtype=tf.float32)\n",
    "    return sampler.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `CategoricalLstmDecoder`는 `BaseLstmDecoder`의 구조를 그대로 가져오면서,   \n",
    "  베르누이 분포를 따르는 $p_\\theta$에 대한 reconstruction error인 cross entropy를 loss로 사용함을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_models.py\n",
    "\n",
    "class HierarchicalLstmDecoder(base_model.BaseDecoder):\n",
    "  \"\"\"Hierarchical LSTM decoder.\"\"\"\n",
    "\n",
    "  def _hierarchical_decode(self, z, base_decode_fn):\n",
    "    \"\"\"Depth first decoding from `z`, passing final embeddings to base fn.\"\"\"\n",
    "    batch_size = z.shape[0]\n",
    "    # Subtract 1 for the core decoder level.\n",
    "    num_levels = len(self._level_lengths) - 1\n",
    "\n",
    "    hparams = self.hparams\n",
    "    batch_size = hparams.batch_size\n",
    "\n",
    "    def recursive_decode(initial_input, path=None):\n",
    "      \"\"\"Recursive hierarchical decode function.\"\"\"\n",
    "      path = path or []\n",
    "      level = len(path)\n",
    "\n",
    "      if level == num_levels:\n",
    "        with tf.variable_scope('core_decoder', reuse=tf.AUTO_REUSE):\n",
    "          return base_decode_fn(initial_input, path)\n",
    "\n",
    "      scope = tf.VariableScope(\n",
    "          tf.AUTO_REUSE, 'decoder/hierarchical_level_%d' % level)\n",
    "      num_steps = self._level_lengths[level]\n",
    "      with tf.variable_scope(scope):\n",
    "        state = lstm_utils.initial_cell_state_from_embedding(\n",
    "            self._hier_cells[level], initial_input, name='initial_state')\n",
    "      if level not in self._disable_autoregression:\n",
    "        # The initial input should be the same size as the tensors returned by\n",
    "        # next level.\n",
    "        if self._hierarchical_encoder:\n",
    "          input_size = self._hierarchical_encoder.level(0).output_depth\n",
    "        elif level == num_levels - 1:\n",
    "          input_size = sum(tf.nest.flatten(self._core_decoder.state_size))\n",
    "        else:\n",
    "          input_size = sum(\n",
    "              tf.nest.flatten(self._hier_cells[level + 1].state_size))\n",
    "        next_input = tf.zeros([batch_size, input_size])\n",
    "      lower_level_embeddings = []\n",
    "      for i in range(num_steps):\n",
    "        if level in self._disable_autoregression:\n",
    "          next_input = tf.zeros([batch_size, 1])\n",
    "        else:\n",
    "          next_input = tf.concat([next_input, initial_input], axis=1)\n",
    "        with tf.variable_scope(scope):\n",
    "          output, state = self._hier_cells[level](next_input, state, scope)\n",
    "        next_input = recursive_decode(output, path + [i])\n",
    "        lower_level_embeddings.append(next_input)\n",
    "      if self._hierarchical_encoder:\n",
    "        # Return the encoding of the outputs using the appropriate level of the\n",
    "        # hierarchical encoder.\n",
    "        enc_level = num_levels - level\n",
    "        return self._hierarchical_encoder.level(enc_level).encode(\n",
    "            sequence=tf.stack(lower_level_embeddings, axis=1),\n",
    "            sequence_length=tf.fill([batch_size], num_steps))\n",
    "      else:\n",
    "        # Return the final state.\n",
    "        return tf.concat(tf.nest.flatten(state), axis=-1)\n",
    "\n",
    "    return recursive_decode(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `HierarchicalLstmDecoder`의 전체 디코딩 동작을 확인했을 때,   \n",
    "  core decoder에서부터 재귀적으로 모든 계층의 디코더를 순회하면서 상위 계층의 $z$로부터 도출되는 state를 모두 종합하여 반환하는 것으로 보입니다.\n",
    "- 위 `hierdec-trio_16bar` 설정에서 4마디를 출력하게 하려면 가장 아랫 계층에 대한 `level_lengths`를 4로 설정해야할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# music_vae_train.py\n",
    "\n",
    "def train(train_dir,\n",
    "          config,\n",
    "          dataset_fn,\n",
    "          checkpoints_to_keep=5,\n",
    "          keep_checkpoint_every_n_hours=1,\n",
    "          num_steps=None,\n",
    "          master='',\n",
    "          num_sync_workers=0,\n",
    "          num_ps_tasks=0,\n",
    "          task=0):\n",
    "  \"\"\"Train loop.\"\"\"\n",
    "  # ...\n",
    "  with tf.Graph().as_default():\n",
    "    with tf.device(tf.train.replica_device_setter(\n",
    "        num_ps_tasks, merge_devices=True)):\n",
    "      model = config.model\n",
    "      model.build(config.hparams,\n",
    "                  config.data_converter.output_depth,\n",
    "                  is_training=True)\n",
    "\n",
    "      optimizer = model.train(**_get_input_tensors(dataset_fn(), config))\n",
    "      # ...\n",
    "      grads, var_list = list(zip(*optimizer.compute_gradients(model.loss)))\n",
    "      global_norm = tf.global_norm(grads)\n",
    "      tf.summary.scalar('global_norm', global_norm)\n",
    "\n",
    "      if config.hparams.clip_mode == 'value':\n",
    "        g = config.hparams.grad_clip\n",
    "        clipped_grads = [tf.clip_by_value(grad, -g, g) for grad in grads]\n",
    "      # ...\n",
    "      train_op = optimizer.apply_gradients(\n",
    "          list(zip(clipped_grads, var_list)),\n",
    "          global_step=model.global_step,\n",
    "          name='train_step')\n",
    "\n",
    "      logging_dict = {'global_step': model.global_step,\n",
    "                      'loss': model.loss}\n",
    "      #...\n",
    "      scaffold = tf.train.Scaffold()\n",
    "      tf_slim.training.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원래 목표로 했던 `train()` 함수로 돌아와, 시작 부분에서 config에 포함된 모델 정보를 바탕으로   \n",
    "  `hparams`와 `data_converter`를 바탕으로 인코더-디코더 구조의 MusicVAE 모델을 생성함을 알 수 있습니다.\n",
    "- 또한, 전처리 과정에서 확인한 `_get_input_tensors()` 함수를 통해 batch size만큼의 데이터를 가져와 학습 데이터로 사용합니다.\n",
    "- optimzer는 reconstruction error인 cross entropy를 최소화하는 방향으로 경사하강법을 수행하면서 최적화를 진행합니다.\n",
    "- Scaffold와 tf_slim의 기능은 완전히 이해하지는 못했지만, 아래 참고 자료를 바탕으로 분산 처리를 위한 구문임을 짐작해 볼 수 있습니다.\n",
    "- 학습 후 평가를 위한 `evaluate()` 함수는 `train()` 함수와 동일한 구조에서 모델 학습에 대한 부분만 제거되었습니다.\n",
    "\n",
    "### References\n",
    "- [Distributed TensorFlow (TensorFlow Dev Summit 2017)](https://youtu.be/la_M6bCV91M)\n",
    "- https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Scaffold\n",
    "- https://digitalbourgeois.tistory.com/51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-style:dotted\">\n",
    "\n",
    "### 생성 과정 탐색\n",
    "- `music_vae_generate.py`를 실행하기 위해선 `run_dir` 또는 `checkpoint_file`, `output_dir`, `mode`를 설정해야 합니다.\n",
    "- `mode`에는 `sample`과 `interpolate`가 있고 샘플 생성을 위해서는 `sample`을 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# music_vae_generate.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "from magenta.models.music_vae import TrainedModel\n",
    "import note_seq\n",
    "\n",
    "flags = tf.app.flags\n",
    "logging = tf.logging\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def run(config_map):\n",
    "  # ...\n",
    "  config = config_map[FLAGS.config]\n",
    "  # ...\n",
    "  logging.info('Loading model...')\n",
    "  if FLAGS.run_dir:\n",
    "    checkpoint_dir_or_path = os.path.expanduser(\n",
    "        os.path.join(FLAGS.run_dir, 'train'))\n",
    "  else:\n",
    "    checkpoint_dir_or_path = os.path.expanduser(FLAGS.checkpoint_file)\n",
    "  model = TrainedModel(\n",
    "      config, batch_size=min(FLAGS.max_batch_size, FLAGS.num_outputs),\n",
    "      checkpoint_dir_or_path=checkpoint_dir_or_path)\n",
    "  # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 생성 과정은 단순하게 모델을 불러와서 샘플링 결과를 저장하는 것입니다.\n",
    "- `run()` 함수의 시작 부분에선 config를 검증하고 `TrainedModel`을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model.py\n",
    "\n",
    "import copy\n",
    "\n",
    "class TrainedModel(object):\n",
    "  def __init__(self, config, batch_size, checkpoint_dir_or_path=None,\n",
    "               var_name_substitutions=None, session_target='', **sample_kwargs):\n",
    "    if tf.gfile.IsDirectory(checkpoint_dir_or_path):\n",
    "      checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir_or_path)\n",
    "    else:\n",
    "      checkpoint_path = checkpoint_dir_or_path\n",
    "    self._config = copy.deepcopy(config)\n",
    "    self._config.data_converter.set_mode('infer')\n",
    "    self._config.hparams.batch_size = batch_size\n",
    "    with tf.Graph().as_default():\n",
    "      model = self._config.model\n",
    "      model.build(\n",
    "          self._config.hparams,\n",
    "          self._config.data_converter.output_depth,\n",
    "          is_training=False)\n",
    "      # Input placeholders\n",
    "      self._temperature = tf.placeholder(tf.float32, shape=())\n",
    "      # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `TrainedModel`의 내부 구조를 알아보기 위해 우선 초기화 메서드를 확인했을 때,   \n",
    "  체크포인트 경로를 받고 `trainable=False` 설정과 함께 모델을 불러와 저장하는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class TrainedModel(object):\n",
    "  def sample(self, n=None, length=None, temperature=1.0, same_z=False,\n",
    "             c_input=None):\n",
    "    \"\"\"Generates random samples from the model.\"\"\"\n",
    "    batch_size = self._config.hparams.batch_size\n",
    "    n = n or batch_size\n",
    "    z_size = self._config.hparams.z_size\n",
    "\n",
    "    if not length and self._config.data_converter.end_token is None:\n",
    "      raise ValueError(\n",
    "          'A length must be specified when the end token is not used.')\n",
    "    length = length or tf.int32.max\n",
    "\n",
    "    feed_dict = {\n",
    "        self._temperature: temperature,\n",
    "        self._max_length: length\n",
    "    }\n",
    "\n",
    "    if self._z_input is not None and same_z:\n",
    "      z = np.random.randn(z_size).astype(np.float32)\n",
    "      z = np.tile(z, (batch_size, 1))\n",
    "      feed_dict[self._z_input] = z\n",
    "\n",
    "    if self._c_input is not None:\n",
    "      feed_dict[self._c_input] = c_input\n",
    "\n",
    "    outputs = []\n",
    "    for _ in range(int(np.ceil(n / batch_size))):\n",
    "      if self._z_input is not None and not same_z:\n",
    "        feed_dict[self._z_input] = (\n",
    "            np.random.randn(batch_size, z_size).astype(np.float32))\n",
    "      outputs.append(self._sess.run(self._outputs, feed_dict))\n",
    "    samples = np.vstack(outputs)[:n]\n",
    "    if self._c_input is not None:\n",
    "      return self._config.data_converter.from_tensors(\n",
    "          samples, np.tile(np.expand_dims(c_input, 0), [batch_size, 1, 1]))\n",
    "    else:\n",
    "      return self._config.data_converter.from_tensors(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 샘플을 생성하는 `sample()` 메서드는 샘플 수 `n`, 샘플 별 최대 길이 `length`, softmax 함수에서의 온도계수 `temperature` 등을 받아,   \n",
    "  `NoteSequence` 타입의 샘플 객체 리스트를 반환합니다.\n",
    "- `z`를 별도로 지정하지 않을 경우 랜덤한 `z`를 생성하고, 전체 샘플 수에 대해 batch size 단위로 반복하며 샘플을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# music_vae_generate.py\n",
    "\n",
    "def run(config_map):\n",
    "  date_and_time = time.strftime('%Y-%m-%d_%H%M%S')\n",
    "  # ...\n",
    "  config = config_map[FLAGS.config]\n",
    "  # ...\n",
    "  model = TrainedModel()\n",
    "  # ...\n",
    "  if FLAGS.mode == 'interpolate':\n",
    "    pass\n",
    "  elif FLAGS.mode == 'sample':\n",
    "    logging.info('Sampling...')\n",
    "    results = model.sample(\n",
    "        n=FLAGS.num_outputs,\n",
    "        length=config.hparams.max_seq_len,\n",
    "        temperature=FLAGS.temperature)\n",
    "\n",
    "  basename = os.path.join(\n",
    "      FLAGS.output_dir,\n",
    "      '%s_%s_%s-*-of-%03d.mid' %\n",
    "      (FLAGS.config, FLAGS.mode, date_and_time, FLAGS.num_outputs))\n",
    "  logging.info('Outputting %d files as `%s`...', FLAGS.num_outputs, basename)\n",
    "  for i, ns in enumerate(results):\n",
    "    note_seq.sequence_proto_to_midi_file(ns, basename.replace('*', '%03d' % i))\n",
    "\n",
    "  logging.info('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다시 `run()` 함수로 돌아와서, 위와 같은 과정을 거쳐 생성된 샘플이 `results`에 담기고,   \n",
    "  `output_dir`에 MIDI 파일이 저장되는 것을 확인할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('mldl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86ae205601b6d906014fa7892090616f7e1469eb0aa86f06d2d1803a695f1eb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
